## Phi-3 Benchmarking

Phi-3 models by Microsoft are relatively small in size, however they have have achieved state-of-the-art results in various NLP tasks, including language translation, text classification, and question-answering. 
Their impressive performance has made them a popular choice for industrial applications and research in AI.

Below are performance charts on Nvidia GPUs using popular llama.cpp engine. Please note that there are many factors and trade-offs when choosing an inference engine for production deployments.

Phi-3-mini-4k-instruct is a 3.8B parameters, lightweight model:
![Phi-3-mini](https://github.com/aimiox/privateAI/blob/main/docs/benchmark/img/nvidia-Phi-3-mini-4k-Q8.png
)

![Phi-3-mini](https://github.com/aimiox/privateAI/blob/main/docs/benchmark/img/nvidia-Phi-3-mini-4k-FP32.png
)

Phi-3-medium-4k-instruct is a 14B parameters, lightweight model:

![Phi-3-mini](https://github.com/aimiox/privateAI/blob/main/docs/benchmark/img/nvidia-Phi-3-medium-4k-Q8.png
)
